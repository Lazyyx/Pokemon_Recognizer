{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0e045b",
   "metadata": {},
   "source": [
    "# Reconnaissance de Pokemon par image\n",
    "\n",
    "### Un peu de contexte\n",
    "\n",
    "Le but de ce projet est de construire un réseau de neurones entièrement connecté capable de reconnaître un Pokemon à partir d'une image donnée.\n",
    "\n",
    "L'implémentation est basée sur PyTorch et le modèle est entrainé et testé sur le dataset PokemonClassification : https://huggingface.co/datasets/keremberke/pokemon-classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "694e5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5258ba",
   "metadata": {},
   "source": [
    "### Étape 1 : Charger les données\n",
    "\n",
    "En PyTorch, les données doivent être transmise au réseau de neurones à l'aide d'un loader. La première étape est de créer une classe de type `Dataset` que la fonction `DataLoader` prend en argument. La classe doit au moins posséder les trois routines `__init__`, `__len__` et `__getitem__`.\n",
    "\n",
    "L'implémentation de cette classe est inspirée du tp1.a, avec une légère modification du `__getitem__` afin d'adapter les dimensions et que le tensor cible soit un scalaire.\n",
    "\n",
    "Pour information, toutes les images chargées sont échantillonnées et réduites en niveaux de gris pour un soucis de rapidité sans en altérer la performance du model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77eb4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoRDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.imgs_path = path\n",
    "        self.class_map = {}\n",
    "        self.transform = transform\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.data=[]\n",
    "        self.img =[]\n",
    "        for i, class_path in enumerate(file_list):\n",
    "            class_name = class_path.split(\"/\")[-1]\n",
    "            for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
    "                self.data.append([img_path, class_name])\n",
    "                img = plt.imread(img_path)\n",
    "                img = img/np.amax(img)\n",
    "                R, G, B = img[::4,::4,0], img[::4,::4,1], img[::4,::4,2]\n",
    "                img = 0.2989 * R + 0.5870 * G + 0.1140 * B # On passe l'image en niveaux de gris pour que ça soit plus rapide\n",
    "                self.img.append(img)\n",
    "            self.class_map[class_name] = i\n",
    "        self.img_dim = (56, 56)    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_name = self.data[idx]\n",
    "        class_id = self.class_map[class_name]\n",
    "        img_tensor = torch.from_numpy(self.img[idx]).unsqueeze(0)  # Add channel dimension\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "        return img_tensor.float(), class_id  # Return scalar class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f37e31",
   "metadata": {},
   "source": [
    "### Étape 2 : Construire son réseau de neurone\n",
    "\n",
    "Il existe deux méthodes pour créer son réseau de neurones :\n",
    "- Créer un module, où les différentes couches et fonctions d'activations sont définies dans la routine `__init__` puis agancée dans la routine `__forward__` pour construire le réseau de neurone;\n",
    "- Dans le cas d'un réseau de neurone classique, en utilisant la fonction `nn.Squential` qui prend comme argument la liste des couches et fonctions d'activations à enchaîner, dans le sens \"forward\".\n",
    "\n",
    "Première méthode :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8598b264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PokemonModel(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=6272, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=110, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PokemonModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PokemonModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.flattened_size = 32 * 14 * 14  # Updated size after pooling\n",
    "        self.fc1 = nn.Linear(self.flattened_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.flattened_size)  # Flatten the tensor\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "pokemonmodel = PokemonModel(len(training_set.class_map))   \n",
    "print(pokemonmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff10c1",
   "metadata": {},
   "source": [
    "Cette méthode devient utile si on veut \"mettre les mains dans le cambouie\". \n",
    "\n",
    "Seconde méthode : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff68f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemonmodel = torch.nn.Sequential(torch.nn.Linear(56 * 56, 512),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Dropout(0.25),\n",
    "                                       torch.nn.Linear(512, 256),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Dropout(0.25),\n",
    "                                       torch.nn.Linear(256, len(training_set.class_map)),\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e965f66",
   "metadata": {},
   "source": [
    "### Étape 3 :  choisir une fonction de coût et une méthode d'optimisation.\n",
    "\n",
    "Ce n'est pas le sujet de ce TP. Dans notre cas nous prenons comme fonction de coût l'entreopie croisée binaire (BCE : *Binary Cross Entropy*) et pour l'optimisation, nous utilisons la descende de gradient stochastique (SGD : *Stochastic Gradient Descent*). Nous verrons au prochain cours pourquoi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2cefe4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(pokemonmodel.parameters(), lr=0.001, weight_decay=10**(-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c8e52",
   "metadata": {},
   "source": [
    "### Étape 4 : création des loader d'entraintement, de validation et de test\n",
    "\n",
    "Ici on choisit un batch de 400, on discutera plus précisément du batch et du réglage de la taille du batch dans le chapitre 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efd35c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(56, scale=(0.8, 1.0)),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "if __name__ == \"__main__\":\n",
    "    training_set = CoRDataset(\"data/training/\", transform=transform)\n",
    "    training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validation_set = CoRDataset(\"data/validation/\")\n",
    "    validation_loader = DataLoader(validation_set, shuffle=False)\n",
    "    test_set = CoRDataset(\"data/test/\")\n",
    "    test_loader = DataLoader(test_set, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f428fd0",
   "metadata": {},
   "source": [
    "On définit ensuite les fonctions d'entrainement et de test `Learning` et `Testmodel`. Commentez ces deux fonctions afin de comprendre leur fonctionnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dac3fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_loss_accuracy(nepoch, tloss, vloss, accuracy, best_tloss, best_vloss, best_accuracy):\n",
    "    print (\"{:<6} {:<15} {:<17} {:<15} {:<20} {:<22} {:<15}\".format(nepoch, tloss, vloss, accuracy, best_tloss, best_vloss, best_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d1c6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Learning(nepoch, model, crit, optim, batchsize, trainingloader, validationloader):\n",
    "    best_tloss = 100.\n",
    "    best_vloss = 100.\n",
    "    best_accuracy = 0.\n",
    "    Print_loss_accuracy('Epoch', 'training loss', 'validation loss', 'accuracy', 'best train loss',\n",
    "                        'best validation loss', 'best accuracy')\n",
    "\n",
    "    for nepoch in range(nepoch):\n",
    "        tloss = 0.\n",
    "        vloss = 0.\n",
    "        correct_test = 0\n",
    "        model.train()\n",
    "\n",
    "        for images, labels in trainingloader:\n",
    "            optim.zero_grad()\n",
    "            images = images.view(images.size(0), -1)  # Flatten the images\n",
    "            predicted = model(images)\n",
    "            loss = crit(predicted, labels)  # Ensure labels are not squeezed\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            tloss += loss.item() * images.size(0)\n",
    "\n",
    "        tloss /= len(trainingloader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validationloader:\n",
    "                images = images.view(images.size(0), -1)  # Flatten the images\n",
    "                predicted = model(images)\n",
    "                loss = crit(predicted, labels)\n",
    "                correct_test += (predicted.argmax(1) == labels).sum().item()\n",
    "                vloss += loss.item() * images.size(0)\n",
    "\n",
    "        vloss /= len(validationloader.dataset)\n",
    "        accuracy = 100 * correct_test / len(validationloader.dataset)\n",
    "\n",
    "        if accuracy >= best_accuracy:\n",
    "            torch.save(model, \"best_model.pth\")\n",
    "            best_accuracy = accuracy\n",
    "        if vloss <= best_vloss:\n",
    "            best_vloss = vloss\n",
    "        if tloss <= best_tloss:\n",
    "            best_tloss = tloss\n",
    "\n",
    "        Print_loss_accuracy(nepoch + 1,\n",
    "                            np.round(tloss, 8),\n",
    "                            np.round(vloss, 8),\n",
    "                            np.round(accuracy, 8),\n",
    "                            np.round(best_tloss, 8),\n",
    "                            np.round(best_vloss, 8),\n",
    "                            np.round(best_accuracy, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d4347f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testmodel(modelfile,crit, testloader):\n",
    "    model = torch.load(modelfile)\n",
    "    model.eval()\n",
    "    plt.figure(dpi=300)\n",
    "    ct=1\n",
    "    for imgs, labels in testloader:\n",
    "        image=imgs[0]\n",
    "        plt.subplot(1, len(test_loader.sampler),ct)\n",
    "        plt.imshow(image)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        predicted = model(imgs.view(1,-1))\n",
    "        test_loss = crit(predicted.squeeze(), labels.squeeze())\n",
    "        plt.title('True label : {} \\n Predicted label : {} \\n Test loss : {}'.format(labels.squeeze().detach().numpy(),\n",
    "                                                                       torch.squeeze(predicted).round().detach().numpy(),\n",
    "                                                                                    np.round(test_loss.item(), 2)),\n",
    "                  fontsize=6)\n",
    "        ct += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e459f066",
   "metadata": {},
   "source": [
    "### À vous de jouer !!\n",
    "\n",
    "En reprenant l'exemple de cellule suivante, essayez de créer un réseau de neuronne entièrement connecté permettant d'identifier chats et lapins. Le model et la méthode d'optimisation doivent être réinitialiser à chaque fois. \n",
    "\n",
    "Vous pouvez rajouter des couches en intercallant la fonction d'activation linéaire rectifiée `nn.ReLU()` (ReLU : *Rectifide Linear Unit*) et couches linéaires `nn.Linear(size_in, size_out)`. \n",
    "\n",
    "Vous pouvez également augmenter le nombre d'epoch et regarder l'effet (ou non).\n",
    "\n",
    "**Attention, le nombre d'entrées d'une couche doit être le nombre de sorties de la couche précédentes !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbf8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  training loss   validation loss   accuracy        best train loss      best validation loss   best accuracy  \n",
      "1      4.65394612      5.03185767        0.0             4.65394612           5.03185767             0.0            \n",
      "2      4.40825871      5.38519916        1.22302158      4.40825871           5.03185767             1.22302158     \n",
      "3      4.24650124      5.63809341        1.43884892      4.24650124           5.03185767             1.43884892     \n",
      "4      4.11095334      5.76879499        0.57553957      4.11095334           5.03185767             1.43884892     \n",
      "5      4.01301803      5.83143888        1.29496403      4.01301803           5.03185767             1.43884892     \n",
      "6      3.90334471      5.92381505        1.72661871      3.90334471           5.03185767             1.72661871     \n",
      "7      3.79933683      5.95649987        2.08633094      3.79933683           5.03185767             2.08633094     \n",
      "8      3.72930061      6.16343608        1.29496403      3.72930061           5.03185767             2.08633094     \n",
      "9      3.67359237      6.30798685        1.65467626      3.67359237           5.03185767             2.08633094     \n"
     ]
    }
   ],
   "source": [
    "Learning(100, pokemonmodel, criterion, optimizer, BATCH_SIZE, training_loader, validation_loader)\n",
    "Testmodel(\"best_model.pth\", criterion, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9cbd9f-1686-424a-affe-05b4f5ebff8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
