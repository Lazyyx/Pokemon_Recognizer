{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0e045b",
   "metadata": {},
   "source": [
    "# Reconnaissance de Pokemon par image\n",
    "\n",
    "### Un peu de contexte\n",
    "\n",
    "Le but de ce projet est de construire un réseau de neurones entièrement connecté capable de reconnaître un Pokemon à partir d'une image donnée.\n",
    "\n",
    "L'implémentation est basée sur PyTorch et le modèle est entrainé et testé sur le dataset PokemonClassification : https://huggingface.co/datasets/keremberke/pokemon-classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "694e5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5258ba",
   "metadata": {},
   "source": [
    "### Étape 1 : Charger les données\n",
    "\n",
    "En PyTorch, les données doivent être transmise au réseau de neurones à l'aide d'un loader. La première étape est de créer une classe de type `Dataset` que la fonction `DataLoader` prend en argument. La classe doit au moins posséder les trois routines `__init__`, `__len__` et `__getitem__`.\n",
    "\n",
    "L'implémentation de cette classe est inspirée du tp1.a, avec une légère modification du `__getitem__` afin d'adapter les dimensions et que le tensor cible soit un scalaire.\n",
    "\n",
    "Pour information, toutes les images chargées sont échantillonnées et réduites en niveaux de gris pour un soucis de rapidité sans en altérer la performance du model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77eb4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoRDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.imgs_path = path\n",
    "        self.class_map = {}\n",
    "        self.transform = transform\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.data=[]\n",
    "        self.img =[]\n",
    "        for i, class_path in enumerate(file_list):\n",
    "            class_name = class_path.split(\"/\")[-1]\n",
    "            for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
    "                self.data.append([img_path, class_name])\n",
    "                img = plt.imread(img_path)\n",
    "                img = img/np.amax(img)\n",
    "                R, G, B = img[::4,::4,0], img[::4,::4,1], img[::4,::4,2]\n",
    "                img = 0.2989 * R + 0.5870 * G + 0.1140 * B # On passe l'image en niveaux de gris pour que ça soit plus rapide\n",
    "                self.img.append(img)\n",
    "            self.class_map[class_name] = i\n",
    "        self.img_dim = (56, 56)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_name = self.data[idx]\n",
    "        class_id = self.class_map[class_name]\n",
    "        img_tensor = torch.from_numpy(self.img[idx]).unsqueeze(0)  # Add channel dimension\n",
    "        if self.transform:\n",
    "            img_tensor = self.transform(img_tensor)\n",
    "        return img_tensor.float(), class_id  # Return scalar class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f37e31",
   "metadata": {},
   "source": [
    "### Étape 2 : Construire son réseau de neurone\n",
    "\n",
    "Il existe deux méthodes pour créer son réseau de neurones :\n",
    "- Créer un module, où les différentes couches et fonctions d'activations sont définies dans la routine `__init__` puis agancée dans la routine `__forward__` pour construire le réseau de neurone;\n",
    "- Dans le cas d'un réseau de neurone classique, en utilisant la fonction `nn.Squential` qui prend comme argument la liste des couches et fonctions d'activations à enchaîner, dans le sens \"forward\".\n",
    "\n",
    "Première méthode :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbff10c1",
   "metadata": {},
   "source": [
    "Cette méthode devient utile si on veut \"mettre les mains dans le cambouie\". \n",
    "\n",
    "Seconde méthode : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff68f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemonmodel = torch.nn.Sequential(torch.nn.Linear(56 * 56, 512),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Dropout(0.6),\n",
    "                                       torch.nn.Linear(512, 256),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Dropout(0.6),\n",
    "                                       torch.nn.Linear(256, 151),\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e965f66",
   "metadata": {},
   "source": [
    "### Étape 3 :  choisir une fonction de coût et une méthode d'optimisation.\n",
    "\n",
    "Ce n'est pas le sujet de ce TP. Dans notre cas nous prenons comme fonction de coût l'entreopie croisée binaire (BCE : *Binary Cross Entropy*) et pour l'optimisation, nous utilisons la descende de gradient stochastique (SGD : *Stochastic Gradient Descent*). Nous verrons au prochain cours pourquoi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cefe4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(pokemonmodel.parameters(), lr=0.0005, weight_decay=10**(-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c8e52",
   "metadata": {},
   "source": [
    "### Étape 4 : création des loader d'entraintement, de validation et de test\n",
    "\n",
    "Ici on choisit un batch de 400, on discutera plus précisément du batch et du réglage de la taille du batch dans le chapitre 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd35c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    #transforms.RandomResizedCrop(56, scale=(0.8, 1.0)),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "if __name__ == \"__main__\":\n",
    "    training_set = CoRDataset(\"data/training/\", transform=transform)\n",
    "    training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validation_set = CoRDataset(\"data/validation/\")\n",
    "    validation_loader = DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_set = CoRDataset(\"data/test/\")\n",
    "    test_loader = DataLoader(test_set, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f428fd0",
   "metadata": {},
   "source": [
    "On définit ensuite les fonctions d'entrainement et de test `Learning` et `Testmodel`. Commentez ces deux fonctions afin de comprendre leur fonctionnement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dac3fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_loss_accuracy(nepoch, tloss, vloss, accuracy, best_tloss, best_vloss, best_accuracy):\n",
    "    print (\"{:<6} {:<15} {:<17} {:<15} {:<20} {:<22} {:<15}\".format(nepoch, tloss, vloss, accuracy, best_tloss, best_vloss, best_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1c6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Learning(nepoch, model, crit, optim, batchsize, trainingloader, validationloader):\n",
    "    best_tloss = 100.\n",
    "    best_vloss = 100.\n",
    "    best_accuracy = 0.\n",
    "    Print_loss_accuracy('Epoch', 'training loss', 'validation loss', 'accuracy', 'best train loss',\n",
    "                        'best validation loss', 'best accuracy')\n",
    "\n",
    "    for nepoch in range(nepoch):\n",
    "        tloss = 0.\n",
    "        vloss = 0.\n",
    "        correct_test = 0\n",
    "        model.train()\n",
    "\n",
    "        for images, labels in trainingloader:\n",
    "            optim.zero_grad()\n",
    "            images = images.view(images.size(0), -1)  # Flatten the images\n",
    "            predicted = model(images)\n",
    "            loss = crit(predicted, labels)  # Ensure labels are not squeezed\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            tloss += loss.item() * images.size(0)\n",
    "\n",
    "        tloss /= len(trainingloader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in validationloader:\n",
    "                images = images.view(images.size(0), -1)  # Flatten the images\n",
    "                predicted = model(images)\n",
    "                loss = crit(predicted, labels)\n",
    "                correct_test += (predicted.argmax(1) == labels).sum().item()\n",
    "                vloss += loss.item() * images.size(0)\n",
    "\n",
    "        vloss /= len(validationloader.dataset)\n",
    "        accuracy = 100 * correct_test / len(validationloader.dataset)\n",
    "\n",
    "        if accuracy >= best_accuracy:\n",
    "            torch.save(model, \"best_model.pth\")\n",
    "            best_accuracy = accuracy\n",
    "        if vloss <= best_vloss:\n",
    "            best_vloss = vloss\n",
    "        if tloss <= best_tloss:\n",
    "            best_tloss = tloss\n",
    "\n",
    "        Print_loss_accuracy(nepoch + 1,\n",
    "                            np.round(tloss, 8),\n",
    "                            np.round(vloss, 8),\n",
    "                            np.round(accuracy, 8),\n",
    "                            np.round(best_tloss, 8),\n",
    "                            np.round(best_vloss, 8),\n",
    "                            np.round(best_accuracy, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d4347f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testmodel(modelfile,crit, testloader):\n",
    "    model = torch.load(modelfile)\n",
    "    model.eval()\n",
    "    plt.figure(dpi=300)\n",
    "    ct=1\n",
    "    for imgs, labels in testloader:\n",
    "        image = imgs[0]\n",
    "        plt.subplot(1, len(testloader), ct)\n",
    "        plt.imshow(image.squeeze(), cmap='gray')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        predicted = model(imgs.view(1, -1))\n",
    "        test_loss = crit(predicted, labels)\n",
    "        plt.title('True label: {}\\nPredicted label: {}\\nTest loss: {}'.format(\n",
    "            labels.item(),\n",
    "            predicted.argmax(1).item(),\n",
    "            np.round(test_loss.item(), 2)),\n",
    "            fontsize=6)\n",
    "        ct += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e459f066",
   "metadata": {},
   "source": [
    "### À vous de jouer !!\n",
    "\n",
    "En reprenant l'exemple de cellule suivante, essayez de créer un réseau de neuronne entièrement connecté permettant d'identifier chats et lapins. Le model et la méthode d'optimisation doivent être réinitialiser à chaque fois. \n",
    "\n",
    "Vous pouvez rajouter des couches en intercallant la fonction d'activation linéaire rectifiée `nn.ReLU()` (ReLU : *Rectifide Linear Unit*) et couches linéaires `nn.Linear(size_in, size_out)`. \n",
    "\n",
    "Vous pouvez également augmenter le nombre d'epoch et regarder l'effet (ou non).\n",
    "\n",
    "**Attention, le nombre d'entrées d'une couche doit être le nombre de sorties de la couche précédentes !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6bbf8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  training loss   validation loss   accuracy        best train loss      best validation loss   best accuracy  \n",
      "1      4.95466735      4.89766796        0.0             4.95466735           4.89766796             0.0            \n",
      "2      4.72554626      4.95586177        2.23021583      4.72554626           4.89766796             2.23021583     \n",
      "3      4.58886837      5.18768203        0.0             4.58886837           4.89766796             2.23021583     \n",
      "4      4.52032023      5.18898315        0.07194245      4.52032023           4.89766796             2.23021583     \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mLearning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpokemonmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m Testmodel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, criterion, test_loader)\n",
      "Cell \u001b[0;32mIn [7], line 14\u001b[0m, in \u001b[0;36mLearning\u001b[0;34m(nepoch, model, crit, optim, batchsize, trainingloader, validationloader)\u001b[0m\n\u001b[1;32m     11\u001b[0m correct_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m trainingloader:\n\u001b[1;32m     15\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mview(images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the images\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn [2], line 29\u001b[0m, in \u001b[0;36mCoRDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     27\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg[idx])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add channel dimension\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 29\u001b[0m     img_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img_tensor\u001b[38;5;241m.\u001b[39mfloat(), class_id\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1372\u001b[0m, in \u001b[0;36mRandomRotation.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         fill \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fill]\n\u001b[1;32m   1370\u001b[0m angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegrees)\n\u001b[0;32m-> 1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/functional.py:1123\u001b[0m, in \u001b[0;36mrotate\u001b[0;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;66;03m# due to current incoherence of rotation angle direction between affine and rotate implementations\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# we need to set -angle.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m matrix \u001b[38;5;241m=\u001b[39m _get_inverse_affine_matrix(center_f, \u001b[38;5;241m-\u001b[39mangle, [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m], \u001b[38;5;241m1.0\u001b[39m, [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m])\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:665\u001b[0m, in \u001b[0;36mrotate\u001b[0;34m(img, matrix, interpolation, expand, fill)\u001b[0m\n\u001b[1;32m    663\u001b[0m theta \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(matrix, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    664\u001b[0m \u001b[38;5;66;03m# grid will be generated on the same device as theta and img\u001b[39;00m\n\u001b[0;32m--> 665\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[43m_gen_affine_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_grid_transform(img, grid, interpolation, fill\u001b[38;5;241m=\u001b[39mfill)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:599\u001b[0m, in \u001b[0;36m_gen_affine_grid\u001b[0;34m(theta, w, h, ow, oh)\u001b[0m\n\u001b[1;32m    596\u001b[0m base_grid[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    598\u001b[0m rescaled_theta \u001b[38;5;241m=\u001b[39m theta\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m w, \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m h], dtype\u001b[38;5;241m=\u001b[39mtheta\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtheta\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 599\u001b[0m output_grid \u001b[38;5;241m=\u001b[39m \u001b[43mbase_grid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrescaled_theta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_grid\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, oh, ow, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Learning(30, pokemonmodel, criterion, optimizer, BATCH_SIZE, training_loader, validation_loader)\n",
    "Testmodel(\"best_model.pth\", criterion, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
