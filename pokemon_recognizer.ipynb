{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0e045b",
   "metadata": {},
   "source": [
    "# Reconnaissance de Pokemon par image\n",
    "\n",
    "### Un peu de contexte\n",
    "\n",
    "Le but de ce projet est de construire un réseau de neurones entièrement connecté capable de reconnaître un Pokemon à partir d'une image donnée.\n",
    "\n",
    "L'implémentation est basée sur PyTorch et le modèle est entrainé et testé sur le dataset PokemonClassification : https://huggingface.co/datasets/keremberke/pokemon-classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694e5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5258ba",
   "metadata": {},
   "source": [
    "### Étape 1 : Charger les données\n",
    "\n",
    "En PyTorch, les données doivent être transmise au réseau de neurones à l'aide d'un loader. La première étape est de créer une classe de type `Dataset` que la fonction `DataLoader` prend en argument. La classe doit au moins posséder les trois routines `__init__`, `__len__` et `__getitem__`.\n",
    "\n",
    "L'implémentation de cette classe est inspirée du tp1.a, avec une légère modification du `__getitem__` afin d'adapter les dimensions et que le tensor cible soit un scalaire.\n",
    "\n",
    "Pour information, toutes les images chargées sont échantillonnées pour un soucis de rapidité sans en altérer la performance du model.\n",
    "\n",
    "Particularité pour le chargement du dataset : nous avons remarqué que l'ordre des pokemon n'étaient pas le même entre chaque partie du dataset (training, test, validation). De plus, certains pokemon sont présents dans la partie testing mais pas dans les données d'entraînement. \n",
    "\n",
    "Ainsi, il a fallu trouver un moyen de faire coincider les labels de tous les Pokemon peu importe la partie du dataset. Pour ce faire, une look-table globale est créée à partir du parcours du dataset entier afin de définir les class map de chaque dataloader à partir des même labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c554b9b-9c6c-4cf9-9ba5-3e2b20226f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon_table = {}\n",
    "for e in [\"test\", \"training\", \"validation\"]:\n",
    "    for i, class_path in enumerate(glob.glob(\"data/\" + e + \"/*\")):\n",
    "        class_name = class_path.split(\"/\")[-1]\n",
    "        if class_name not in pokemon_table.keys():\n",
    "            pokemon_table[class_name] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77eb4ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoRDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.imgs_path = path\n",
    "        self.class_map = {}\n",
    "        self.transform = transform\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "        self.data=[]\n",
    "        self.img =[]\n",
    "        for i, class_path in enumerate(file_list):\n",
    "            class_name = class_path.split(\"/\")[-1]\n",
    "            for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
    "                self.data.append([img_path, class_name])\n",
    "                img = plt.imread(img_path)\n",
    "                img = img/np.amax(img)\n",
    "                R, G, B = img[::4,::4,0], img[::4,::4,1], img[::4,::4,2]\n",
    "                #img = 0.2989 * R + 0.5870 * G + 0.1140 * B # On passe l'image en niveaux de gris pour que ça soit plus rapide\n",
    "                img = np.stack([R, G, B], 2)\n",
    "                self.img.append(img)\n",
    "            self.class_map[class_name] = pokemon_table[class_name]\n",
    "        self.img_dim = (3, 56, 56)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_name = self.data[idx]\n",
    "        class_id = self.class_map[class_name]\n",
    "        img_tensor = torch.from_numpy(self.img[idx])\n",
    "        img_tensor = img_tensor.permute(2, 0, 1)\n",
    "        class_id = torch.tensor([class_id])\n",
    "        return img_tensor.float(), class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f37e31",
   "metadata": {},
   "source": [
    "### Étape 2 : Construire son réseau de neurone\n",
    "\n",
    "Nous avons décidé d'implémenter un réseau de neurone classique, en utilisant la fonction `nn.Sequential` qui prend comme argument la liste des couches avec leur nombre de neurones associés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff68f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemonmodel = torch.nn.Sequential(torch.nn.Linear(3 * 56 * 56, 512),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Dropout(0.6),\n",
    "                                       torch.nn.Linear(512, 256),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Dropout(0.6),\n",
    "                                       torch.nn.Linear(256, 110),\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e965f66",
   "metadata": {},
   "source": [
    "### Étape 3 :  choisir une fonction de coût et une méthode d'optimisation.\n",
    "\n",
    "Dans notre cas nous prenons comme fonction de coût l'entreopie croisée et pour l'optimisation, nous utilisons la descende de gradient stochastique (SGD : *Stochastic Gradient Descent*). Nous verrons également plus loin une implémentation variant entre Adam, SGD et Adagrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cefe4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(pokemonmodel.parameters(), lr=0.0005, weight_decay=10**(-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c8e52",
   "metadata": {},
   "source": [
    "### Étape 4 : création des loader d'entraintement, de validation et de test\n",
    "\n",
    "Ici on choisit un batch de 200, le plus efficace (ratio temps/précision) au vu de nos tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd35c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "if __name__ == \"__main__\":\n",
    "    training_set = CoRDataset(\"data/training/\")\n",
    "    training_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validation_set = CoRDataset(\"data/validation/\")\n",
    "    validation_loader = DataLoader(validation_set, shuffle=False)\n",
    "    test_set = CoRDataset(\"data/test/\")\n",
    "    test_loader = DataLoader(test_set, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f428fd0",
   "metadata": {},
   "source": [
    "On définit ensuite la fonction d'entrainement `Learning` ainsi qu'une fonction utilitaire d'affichage de performance `Print_loss_accuracy`.\n",
    "Cette fonction sont inspirées de l'implémentation du TP1.a à l'exception de la méthode `view` qui, ici, ne prend pas la batchsize car on souhaite \"aplatir\" l'image (flatten). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dac3fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Print_loss_accuracy(nepoch, tloss, vloss, accuracy, best_tloss, best_vloss, best_accuracy):\n",
    "    print (\"{:<6} {:<15} {:<17} {:<15} {:<20} {:<22} {:<15}\".format(nepoch, tloss, vloss, accuracy, best_tloss, best_vloss, best_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1c6864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Learning(nepoch, model, crit, optim, batchsize, trainingloader, validationloader, algo, learning_rate, decay):\n",
    "    writer = SummaryWriter(log_dir='runs/' + str(algo) + ': lr=' + str(learning_rate)+', decay = ' + str(decay))\n",
    "    best_tloss = 100.\n",
    "    best_vloss = 100.\n",
    "    best_accuracy = 0.\n",
    "    Print_loss_accuracy('Epoch', 'training loss', 'validation loss', 'accuracy', 'best train loss',\n",
    "                        'best validation loss', 'best accuracy')\n",
    "\n",
    "    for nepoch in range(nepoch):\n",
    "        tloss = 0.\n",
    "        vloss = 0.\n",
    "        correct_test = 0\n",
    "        model.train()\n",
    "\n",
    "        for images, labels in trainingloader:\n",
    "            optim.zero_grad()\n",
    "            #print(images.size(0))\n",
    "            #print(images.view(batchsize, -1).size())\n",
    "            predicted = model(images.view(images.size(0), -1))\n",
    "            loss = crit(predicted.squeeze(), labels.squeeze())\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            tloss += loss.item() * images.size(0)\n",
    "\n",
    "        tloss /= len(trainingloader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for images, labels in validationloader:\n",
    "            predicted = model(images.view(1, -1))\n",
    "            loss = crit(predicted.squeeze(), labels.squeeze())\n",
    "            correct_test += (predicted.argmax(1) == labels).sum().item()\n",
    "            vloss += loss.item() * images.size(0)\n",
    "\n",
    "        vloss /= len(validationloader.dataset)\n",
    "        accuracy = 100 * correct_test / len(validationloader.dataset)\n",
    "        writer.add_scalar('Accuracy', accuracy, nepoch)\n",
    "        writer.add_scalar('Loss/test', tloss, nepoch)\n",
    "        writer.add_scalar('Loss/validation', vloss, nepoch)\n",
    "\n",
    "        if accuracy >= best_accuracy:\n",
    "            torch.save(model, \"best_model.pth\")\n",
    "            best_accuracy = accuracy\n",
    "        if vloss <= best_vloss:\n",
    "            best_vloss = vloss\n",
    "        if tloss <= best_tloss:\n",
    "            best_tloss = tloss\n",
    "\n",
    "        Print_loss_accuracy(nepoch + 1,\n",
    "                            np.round(tloss, 8),\n",
    "                            np.round(vloss, 8),\n",
    "                            np.round(accuracy, 8),\n",
    "                            np.round(best_tloss, 8),\n",
    "                            np.round(best_vloss, 8),\n",
    "                            np.round(best_accuracy, 8))\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e459f066",
   "metadata": {},
   "source": [
    "### Let's train !!!\n",
    "\n",
    "Il ne reste plus qu'à entrainer le modèle à partir des Dataloader précédemment crées !\n",
    "\n",
    "On décide ici d'effectuer 50 epochs, à ajuster selon besoin.\n",
    "\n",
    "**La fonction d'entrainement enregistre les performances de chaque itération sur un Tensorboard dont les logs sont enregistrés sous le dossier `runs`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6bbf8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  training loss   validation loss   accuracy        best train loss      best validation loss   best accuracy  \n",
      "1      4.71397593      4.7146829         0.0             4.71397593           4.7146829              0.0            \n",
      "2      4.71078858      4.71459272        0.0             4.71078858           4.71459272             0.0            \n",
      "3      4.70966286      4.71408475        0.07194245      4.70966286           4.71408475             0.07194245     \n",
      "4      4.70859163      4.71353704        0.0             4.70859163           4.71353704             0.07194245     \n",
      "5      4.70616248      4.71359661        0.0             4.70616248           4.71353704             0.07194245     \n",
      "6      4.70372557      4.7129276         0.07194245      4.70372557           4.7129276              0.07194245     \n",
      "7      4.70671889      4.71262293        0.14388489      4.70372557           4.71262293             0.14388489     \n",
      "8      4.70613202      4.7122704         0.07194245      4.70372557           4.7122704              0.14388489     \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mLearning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpokemonmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSGD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [8], line 30\u001b[0m, in \u001b[0;36mLearning\u001b[0;34m(nepoch, model, crit, optim, batchsize, trainingloader, validationloader, algo, learning_rate, decay)\u001b[0m\n\u001b[1;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m validationloader:\n\u001b[0;32m---> 30\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m crit(predicted\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m     32\u001b[0m     correct_test \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Learning(50, pokemonmodel, criterion, optimizer, BATCH_SIZE, training_loader, validation_loader, \"SGD\", 0.005, 10**(-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1951c-b51b-4b82-b885-de92f0ee7c9f",
   "metadata": {},
   "source": [
    "### Optimisons tout ça !!\n",
    "\n",
    "Maintenant que notre réseau de neurones est fonctionnel, il est temps d'optimiser ses performances.\n",
    "\n",
    "Pour cela, on cherche à faire varier :\n",
    "- La fonction d'activation (entre SGD, Adam, Adagrad).\n",
    "- Le weight decay (dont le but est d'ajouter une pénalité au model en cas d'erreur).\n",
    "- Le learning rate (qui détermine la taille du pas à chaque itération).\n",
    "\n",
    "Pour ne pas biaiser l'apprentissage entre 2 itérations, le modèle est reinitialisé à chaque entraînement via `copy.deepcopy`.\n",
    "\n",
    "Pour un soucis de rapidité, le nombre d'epoch ainsi que chaque choix d'itérations peut être modifié comme souhaité.\n",
    "\n",
    "**Ces itérations sur différentes valeurs est particulièrement utile afin de se rendre compte des hyperparamètres optimaux. (Voir rapport pour courbes et analyse de performance)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b6c853-7fe7-4edb-bc1f-03f06f99f40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model = copy.deepcopy(pokemonmodel)\n",
    "for opti in [\"SGD\", \"Adam\", \"Adagrad\"]:\n",
    "    for val in [0, 10 ** (-5), 10 ** (-2), 10 ** 0, 10 ** 2]:\n",
    "        for learning_rate in [0.01, 0.001, 0.005]:\n",
    "            pokemonmodel = copy.deepcopy(initial_model)\n",
    "            optimizer = torch.optim.Adam(pokemonmodel.parameters(), lr=learning_rate, weight_decay=val)\n",
    "            if opti == \"SGD\":\n",
    "                optimizer = torch.optim.SGD(pokemonmodel.parameters(), lr=learning_rate, weight_decay=val)\n",
    "            elif opti == \"Adagrad\":\n",
    "                optimizer = torch.optim.Adagrad(pokemonmodel.parameters(), lr=learning_rate, weight_decay=val)\n",
    "            Learning(25, pokemonmodel, criterion, optimizer, BATCH_SIZE, training_loader, validation_loader, opti, learning_rate, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab4fa4-2fe1-4710-892a-0a7563a81cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
